---
title: "Bitcoin"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=TRUE, message=FALSE, warning=FALSE}
#load libraries
library(ggplot2)
library(dplyr)
library(caret)
library(randomForest)
library(cowplot)
library(pROC)

```


```{r, echo=TRUE, message=FALSE, warning=FALSE}
#load data
bitcoin <- read.csv("BitcoinHeistData.csv")

head(bitcoin)

summary(bitcoin)
```
```{r, echo=TRUE, message=FALSE, warning=FALSE}
bitcoin%>%
  group_by(label)%>%
  count()

```

There are 3 families of Rasomware: Montreal, Padua and Princeton that we are going to group in a label "rasomware" to treat this problem as a binary classification


```{r, echo=TRUE, message=FALSE, warning=FALSE}
#create a binary level variable for transactions with any type of Rasomware or white transactions
bitcoin <- bitcoin%>%
  mutate(b_label = if_else(label== "white", "noRasomware", "Rasomware"))

```

## EDA

```{r, echo=TRUE, message=FALSE, warning=FALSE}

#check number of transactions white and rasomware
print(prop.table(table(bitcoin$b_label)))

ggplot(data = bitcoin, aes(x=b_label, fill= b_label)) + geom_bar(stat = 'count') 
```
The dataset is highly unbalanced 98.5 % of data are labeled as white transactions

```{r echo=TRUE, message=FALSE, warning=FALSE}
#check missing values
sum(is.na(bitcoin))


#boxplot to check for outliers
weight_p <- ggplot(data = bitcoin, aes (x=weight, color=b_label)) + geom_boxplot() + facet_wrap(~b_label)
len_p <- ggplot(data = bitcoin, aes (x=length, color=b_label)) + geom_boxplot() + facet_wrap(~b_label)
count_p <- ggplot(data = bitcoin, aes (x=count, color=b_label)) + geom_boxplot() + facet_wrap(~b_label)
loop_p <- ggplot(data = bitcoin, aes (x=looped, color=b_label)) + geom_boxplot() + facet_wrap(~b_label)
label_p <- ggplot(data = bitcoin, aes (x=neighbors, color=b_label)) + geom_boxplot() + facet_wrap(~b_label)
income_p <- ggplot(data = bitcoin, aes (x=income, color=b_label)) + geom_boxplot() + facet_wrap(~b_label)

cowplot::plot_grid(weight_p, len_p, count_p, loop_p, label_p, income_p, labels = "AUTO", ncol = 2)
```

There are no missing values, but we can notice there are some outliers

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(bitcoin) + aes(x = b_label, y = income, color=b_label) + geom_jitter()

```

The date is given by the year and the number of day in the year. We create a new variable of date in the format YYYY-mm-dd

```{r echo=TRUE, message=FALSE, warning=FALSE}


bitcoin$ref_date <- paste(as.character(bitcoin$year-1),"-12-31", sep="")

bitcoin$new_date <- as.Date(bitcoin$day, origin= bitcoin$ref_date)

ggplot(data = bitcoin, aes(x=new_date, y= income)) + geom_line() + facet_wrap(~b_label)

```
```{r echo=TRUE, message=FALSE, warning=FALSE}
#bitcoin transactions related to any type of Rasomware
tab_r <- table(bitcoin %>%
  filter(b_label=="Rasomware") %>%
  select(year))

plot(tab_r)
```


### Outliers

Boxplots of some variables shown the presence of outliers.

```{r echo=TRUE, message=FALSE, warning=FALSE}
#removing outliers in weights
weight_out <- boxplot(bitcoin$weight, plot = FALSE)$out

new_bitcoin <- bitcoin
new_bitcoin <- new_bitcoin[-which(new_bitcoin$weight %in% weight_out),]

ggplot(data = new_bitcoin, aes (x=weight, color=b_label)) + geom_boxplot() + facet_wrap(~b_label)
``` 


```{r, echo=TRUE, message=FALSE, warning=FALSE}

#remove neighbors outliers

neigh_out <- boxplot(bitcoin$neighbors, plot=FALSE)$out

new_bitcoin <- new_bitcoin[-which(new_bitcoin$neighbors %in% neigh_out),]

ggplot(data = new_bitcoin, aes (x=neighbors, color=b_label)) + geom_boxplot() + facet_wrap(~b_label)
```


```{r, echo=TRUE, message=FALSE, warning=FALSE}

income_out <- boxplot(new_bitcoin$income)$out

new_bitcoin <- new_bitcoin[-which(new_bitcoin$income %in% income_out), ]

ggplot(data = new_bitcoin, aes(x=income, color = b_label)) + geom_boxplot() + facet_wrap(~ b_label)

```

```{r, echo=TRUE, message=FALSE, warning=FALSE}

print(paste("Before cleaning dataset contains " ,dim(bitcoin)[1]))
print(paste("After cleaning dataset contains " ,dim(new_bitcoin)[1]))

print("class proportion before")
print(prop.table(table(bitcoin$b_label)))

print("class proportion after")
print(prop.table(table(new_bitcoin$b_label)))

print(paste("number of instances removed", dim(bitcoin)[1]-dim(new_bitcoin)[1]))
```

After cleaning we can notice there were a lot of data removed due to outliers. However, the proportion of classes is not widely altered.

### Correlation

```{r, echo=TRUE, message=FALSE, warning=FALSE}
new_bitcoin$new_label <-as.factor(ifelse(new_bitcoin$b_label == "Rasomware", 1, 0))
cor(new_bitcoin[c("length", "weight", "count", "looped", "neighbors", "income")])

```


### Normalization

```{r, echo=TRUE, message=FALSE, warning=FALSE}
par(mfcol=c(2,1))
hist(new_bitcoin$income)
hist(log(new_bitcoin$income))

```

```{r, echo=TRUE, message=FALSE, warning=FALSE}

par(mfcol = c(2,1))
hist(new_bitcoin$length)
hist(log(new_bitcoin$length))
```
```{r, echo=TRUE, message=FALSE, warning=FALSE}

par(mfcol = c(2,1))
hist(new_bitcoin$weight)
hist(log(new_bitcoin$weight))
```
```{r, echo=TRUE, message=FALSE, warning=FALSE}

par(mfcol = c(2, 1))
hist(new_bitcoin$neighbors)
hist(log(new_bitcoin$neighbors))

```


```{r, echo=TRUE, message=FALSE, warning=FALSE}
#https://github.com/dataprofessor/code/blob/master/iris/iris-classification.R

normalize <- function(x){
  return((x-min(x))/(max(x)-min(x)))
}

#copy of a dataframe to be normalized for Logistic Regression

df <- new_bitcoin[c("length", "weight", "count", "looped", "neighbors", "income", "new_label")]

df$income <- log(df$income)

df$length <- normalize(df$length)
df$weight <- normalize(df$weight)
df$count <- normalize(df$count)
df$looped <- normalize(df$looped)
df$neighbors <- normalize(df$neighbors)
df$income <- normalize(df$income)

set.seed(100)
train_index <- createDataPartition(df$new_label, p=0.8, list = FALSE)

train_df <- df[train_index,]
test_df <- df[-train_index,]


```

## Logistic Regression

```{r, echo=TRUE, warning=FALSE, message=FALSE}
glm_model <- glm(new_label ~ ., family = binomial, data= train_df)
summary(glm_model)

glm_pred <- predict(glm_model, test_df, type = "response")

roc(test_df$new_label, glm_pred, plot = TRUE, print.auc = TRUE, c1.alpha=TRUE)

glm_cm <- confusionMatrix(factor(ifelse(glm_pred>0.02,1,0), levels=c(0,1)), factor(test_df$new_label))
glm_cm

```


The model summary shows that length and count are not significant predictors of the model for alpha of 0.05

The results of the model shows that due to the highly imbalanced data the results are not great, it fails to detect rasomware cases. Therefore we are going to do some tests with a subset of data with a proportion of 80% white and 20% ransomware

```{r, echo=TRUE, warning=FALSE, message=FALSE}

prop.table(table(train_df$new_label))
prop.table(table(test_df$new_label))

df_ransomware <- df %>% 
  filter(new_label == 1)

df_white <- df %>% 
  filter(new_label == 0) 

new_df <- rbind(df_ransomware, df_white)

new_df <- new_df %>% sample_n(182025)
set.seed(100)
new_train_index <- createDataPartition(new_df$new_label, p=0.8, list = FALSE)

new_train_df <- new_df[new_train_index,]
new_test_df <- new_df[-new_train_index,]

#check proportion classes between 
prop.table(table(new_train_df$new_label))
prop.table(table(new_test_df$new_label))
```

```{r, echo=TRUE, warning=FALSE, message=FALSE}

glm_model_1 <- glm(new_label ~ ., data = new_train_df, family = binomial )

glm_pred_1 <-predict(glm_model_1, new_test_df, type = 'response')

roc(new_test_df$new_label, glm_pred_1, c1.alpha = TRUE, plot = TRUE, print.auc = TRUE)

glm_1_cm <- confusionMatrix(factor(ifelse(glm_pred_1>0.020,1,0), levels=c(0,1)), factor(new_test_df$new_label))
glm_1_cm
```

We did some tests using SVM and Random Forest, but due to the size of the dataset we ran into memory issues 


